{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as p\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "my_stopwords = nltk.corpus.stopwords.words('english')\n",
    "word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n",
    "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@â'\n",
    "\n",
    "os.path.abspath(os.getcwd())\n",
    "os.chdir('K:/UIC/Projects/Twitter/Final')\n",
    "input_path = 'K:/UIC/Projects/Twitter/Final'\n",
    "output_path = 'K:/UIC/Projects/Twitter/Final/cleaned'\n",
    "\n",
    "IMeMyself=['i','im', 'me', 'my', 'mine', 'myself', 'she', 'he', 'her', 'him', 'you', 'thou', 'your', 'have', 'has', 'we']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Side Effect Cleaning Function:\n",
    "\n",
    "List_SideEffects = []\n",
    "for i in range(len(All_Side_Effects)):\n",
    "    SE = All_Side_Effects.SE[i]\n",
    "    \n",
    "    SE = re.sub('['+my_punctuation + ']+', ' ', SE) # strip punctuation\n",
    "    SE = re.sub('\\s+', ' ', SE) #remove double spacing\n",
    "    SE = re.sub('([0-9]+)', '', SE) # remove numbers\n",
    "    SE_token_list = [word for word in SE.split(' ') if word not in my_stopwords] # remove stopwords\n",
    "    SE_token_list = [lemmatizer.lemmatize(word) if '#' not in word else word\n",
    "                        for word in SE_token_list]\n",
    "    #SE_token_list = [word_rooter(word) if '#' not in word else word\n",
    "    #                    for word in SE_token_list] # apply word rooter'''\n",
    "    SE = ' '.join(SE_token_list)\n",
    "    if SE not in List_SideEffects: List_SideEffects.append(SE)\n",
    "    #All_SideEffects.append(SE)\n",
    "len(All_Side_Effects), len(List_SideEffects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drug Specific Side Effects Cleaning Function:\n",
    "\n",
    "def DrugSEClean (csv):\n",
    "    Drug_SE=p.DataFrame(p.read_excel(csv))\n",
    "    Drug1_SideEffects = []\n",
    "    for i in range(len(Drug_SE)):\n",
    "            SE = Drug_SE.SE[i]\n",
    "\n",
    "            SE = re.sub('['+my_punctuation + ']+', ' ', SE) # strip punctuation\n",
    "            SE = re.sub('\\s+', ' ', SE) #remove double spacing\n",
    "            SE = re.sub('([0-9]+)', '', SE) # remove numbers\n",
    "            SE_token_list = [word for word in SE.split(' ') if word not in my_stopwords] # remove stopwords\n",
    "            SE_token_list = [lemmatizer.lemmatize(word) if '#' not in word else word\n",
    "                                for word in SE_token_list]\n",
    "            '''SE_token_list = [word_rooter(word) if '#' not in word else word\n",
    "                                for word in SE_token_list] # apply word rooter'''\n",
    "            SE = ' '.join(SE_token_list)\n",
    "            SE=SE.strip()\n",
    "            if SE not in Drug1_SideEffects: Drug1_SideEffects.append(SE)\n",
    "    return Drug1_SideEffects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Function and sorting ALgo:\n",
    "\n",
    "\n",
    "All_Side_Effects = p.DataFrame(p.read_excel(\"ALL_SideEffects.xlsx\"))\n",
    "List_SideEffects = []\n",
    "for i in range(len(All_Side_Effects)):\n",
    "    SE = All_Side_Effects.SE[i]\n",
    "    SE = re.sub('['+my_punctuation + ']+', ' ', SE) \n",
    "    SE = re.sub('\\s+', ' ', SE) \n",
    "    SE = re.sub('([0-9]+)', '', SE) \n",
    "    SE_token_list = [word for word in SE.split(' ') if word not in my_stopwords] \n",
    "    SE_token_list = [lemmatizer.lemmatize(word) if '#' not in word else word\n",
    "    SE = ' '.join(SE_token_list)\n",
    "    SE=SE.strip()\n",
    "    if SE not in List_SideEffects: List_SideEffects.append(SE)\n",
    "def DrugSEClean (csv):\n",
    "    Drug_SE=p.DataFrame(p.read_excel(csv))\n",
    "    Drug1_SideEffects = []\n",
    "    for i in range(len(Drug_SE)):\n",
    "            SE = Drug_SE.SE[i]\n",
    "\n",
    "            SE = re.sub('['+my_punctuation + ']+', ' ', SE) \n",
    "            SE = re.sub('\\s+', ' ', SE) \n",
    "            SE = re.sub('([0-9]+)', '', SE) \n",
    "            SE_token_list = [word for word in SE.split(' ') if word not in my_stopwords] \n",
    "            SE_token_list = [lemmatizer.lemmatize(word) if '#' not in word else word\n",
    "                                for word in SE_token_list]\n",
    "            SE = ' '.join(SE_token_list)\n",
    "            if SE not in Drug1_SideEffects: Drug1_SideEffects.append(SE)\n",
    "    return Drug1_SideEffects\n",
    "def remove_links(tweet):\n",
    "    '''Takes a string and removes web links from it'''\n",
    "    tweet = re.sub(r'http\\S+', '', tweet) \n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet) \n",
    "    tweet = tweet.strip('[link]') \n",
    "    return tweet\n",
    "def remove_users(tweet):\n",
    "    '''Takes a string and removes retweet and @user information'''\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
    "    return tweet\n",
    "def reduce_lengthening(word):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", word)\n",
    "from autocorrect import Speller\n",
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')\n",
    "# cleaning master function\n",
    "def clean_tweets (tweet, bigrams=False):\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = tweet.lower() # lower case\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) \n",
    "    tweet = re.sub('['+my_punctuation + ']+', ' ', tweet) \n",
    "    tweet = re.sub('\\s+', ' ', tweet) \n",
    "    tweet = re.sub('([0-9]+)', '', tweet) \n",
    "    tweet_token_list = [word for word in tweet.split(' ') if word not in my_stopwords] \n",
    "    tweet_token_list = [lemmatizer.lemmatize(word) if '#' not in word else word\n",
    "                        for word in tweet_token_list]\n",
    "    if bigrams:\n",
    "        tweet_token_list = tweet_token_list+[tweet_token_list[i]+'_'+tweet_token_list[i+1]\n",
    "                                            for i in range(len(tweet_token_list)-1)]\n",
    "    tweet = ' '.join(tweet_token_list)\n",
    "    return tweet\n",
    "def clean_tokens (tweet):\n",
    "    tweet_token_list = [word for word in tweet.split(' ') if word not in my_stopwords]\n",
    "    while ('') in tweet_token_list: tweet_token_list.remove('')\n",
    "    return tweet_token_list\n",
    "def count_prescribed (cleaned_tocken):\n",
    "    j=0\n",
    "    prescribed_token = []\n",
    "    for i in range(len(cleaned_tocken)):\n",
    "        if ((cleaned_tocken[i] in Drug1_SideEffects) and (cleaned_tocken[i] not in prescribed_token)):\n",
    "            j+=1\n",
    "            prescribed_token.append(cleaned_tocken[i])\n",
    "    Dict = {j : sorted(prescribed_token)}\n",
    "    return Dict\n",
    "def count_UNprescribed (cleaned_tocken):\n",
    "    j=0\n",
    "    UNprescribed_token = []\n",
    "    for i in range(len(cleaned_tocken)):\n",
    "        if ((cleaned_tocken[i] in List_SideEffects) and (cleaned_tocken[i] not in Drug1_SideEffects) and (cleaned_tocken[i] not in UNprescribed_token)):\n",
    "            j+=1\n",
    "            UNprescribed_token.append(cleaned_tocken[i])\n",
    "    Dict = {j : sorted(UNprescribed_token)}\n",
    "    return Dict\n",
    "def org_check (tweet):\n",
    "    tweet = tweet.lower() \n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) \n",
    "    tweet = re.sub('['+my_punctuation + ']+', ' ', tweet) \n",
    "    tweet_token_list = [word for word in tweet.split(' ')]\n",
    "    for word in tweet_token_list:\n",
    "        if (word in IMeMyself):\n",
    "            a=0\n",
    "            break\n",
    "        else: a=1\n",
    "    return a\n",
    "def clean_master (df, Org_1, Clean_Tweets, Clean_Tweet_Tokens, Prescribed_Key_Value, UNPrescribed_Key_Value):\n",
    "    #df\n",
    "    print(\"Cleaning Started\")\n",
    "    df[Org_1]=df.Tweet.apply(org_check)\n",
    "    print (\"Organization Checked\")\n",
    "    df[Clean_Tweets]=df.Tweet.apply(clean_tweets)\n",
    "    print (\"Tweets Cleaned\")\n",
    "    df[Clean_Tweet_Tokens]=df.Clean_Tweets.apply(clean_tokens)\n",
    "    print (\"Cleaned Tweets Tocanised\")\n",
    "    df[Prescribed_Key_Value] = df.Clean_Tweet_Tokens.apply(count_prescribed)\n",
    "    print (\"Key Value pair for Prescribed Prepared\")\n",
    "    df[UNPrescribed_Key_Value] = df.Clean_Tweet_Tokens.apply(count_UNprescribed)\n",
    "    print (\"Key Value pair for UN Prescribed Prepared\")\n",
    "    return df\n",
    "def drugClean(UFfile, SEfile, OPname):\n",
    "    Data=p.DataFrame(p.read_csv(UFfile))\n",
    "    Drug1_SideEffects = DrugSEClean(SEfile)\n",
    "    Clean_Data = clean_master(Data,\"Org1\",\"Clean_Tweets\", \"Clean_Tweet_Tokens\", \"Prescribed_Key_Value\", \"UNPrescribed_Key_Value\")\n",
    "    return (Clean_Data.to_excel(r'K:/UIC/Projects/Twitter/Final/cleaned/'+str(OPname)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Drug1_SideEffects=DrugSEClean('')\n",
    "drugClean('_6Warfarin_UF.csv','_6Warfarin_SE.xlsx','temp.xlsx') #(Input File, SideEffect File, Output File)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
